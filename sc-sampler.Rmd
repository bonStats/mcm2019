---
title: "Scaling samplers for high-dimensional models with stochastic nets"
subtitle: "<br><br>"
author: "Joshua J Bon<br>Queensland University of Technology<br>MCM2019 -- July 11th"
institute: ""
date: ""
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts","bon-beach-title.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      
---

```{r setup, load_refs, include=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "html",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("bib-sc-sampler.bib", check = FALSE)

library(ggplot2)
source("one-dim-sc-example-viz.R")

```

## Talk outline
<br>
1. **Bayesian Lasso**: An interesting reinterpretation (at least to me). 
<br><br><br>
2. **Stochastic constraints variables**: A good name?
<br><br><br>
3. **Stochastic nets**: Continuous shrinkage priors as stochastic constraints.
<br><br><br>
4. **Geometry**: I'll show some pictures and wave my hands.
<br><br><br>
5. **A new Gibbs sampler**: A bit speedy, more stable. *Or the MC part*.

---

## The Lasso 

```{r aqua-lasso-1, echo=FALSE, eval=TRUE}
knitr::include_graphics("imgs/aquaman-hammerhead-1967-cartoon.JPG")
```

<!--- If aquaman can use a lasso under water, what can I do? "Weird" ---> 

---

## The standard Lasso

### Lagrangian form

$$\max_{\boldsymbol{\theta} \in \mathbb{R}^{p}} \log \pi(\boldsymbol{x}|\boldsymbol{\theta}) + \lambda \Vert \boldsymbol{\theta} \Vert_{1}$$

### Constrained form

$$			\max_{\tilde{\boldsymbol{\theta}} \in \mathbb{R}^{p}} \log \pi(\boldsymbol{x}|\tilde{\boldsymbol{\theta}}) \quad\text{ s.t. } \quad \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}$$

```{r tib-ref, echo=FALSE, eval=FALSE, results='asis'}
cat("<br><br><br> see ",Citet("Tibshirani1996", bib = myBib))
```

---

## The Bayesian Lasso

### Standard probabilistic model:

$$(\boldsymbol{x}|\boldsymbol{\theta}) \sim \pi(\boldsymbol{x}|\boldsymbol{\theta})$$
$$\boldsymbol{\theta} \overset{\text{iid}}{\sim} \text{DExp}(\lambda)$$

### Constrained probabilistic model:

$$(\boldsymbol{x} \vert \boldsymbol{\theta}) \sim \pi(\boldsymbol{x} \vert \boldsymbol{\theta}) $$

$$(\boldsymbol{\theta},\omega) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}},\tilde{\omega}\;\; \text{s.t.}\;\; \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}) $$

$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad \tilde{\omega} \sim \text{Exp}(\lambda)$$
<!--- More complicated, but exposes inner facets, Marginalising over $\omega$ results in the original Bayesian Lasso. --->

---

## The constrained Bayesian Lasso

This looks suspiciously like changing the constrained Lasso
<br><br>
$$\max_{\tilde{\boldsymbol{\theta}} \in \mathbb{R}^{p}} \log \pi(\boldsymbol{x} \vert \tilde{\boldsymbol{\theta}}) \quad\text{ s.t. } \quad \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}$$
<br>
to a Bayesian problem by setting priors
<br><br>
$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1 \qquad
\tilde{\omega} \sim \text{Exp}(\lambda)\qquad
\text{s.t.}\qquad \Vert \tilde{\boldsymbol{\theta}} \Vert_{1} \leq \tilde{\omega}$$
<br>
just as we had with stochastic constraints.
<!--- 
	\item Closer connection between Bayesian and standard Lasso
	\item Connected by more than just the MAP
\end{itemize} --->


---

## Duality and analogy to regularisation
<br><br><br>

| *Inference*  | *Regularisation*                                          |
|--------------|-----------------------------------------------------------|
| Optimisation | Penalty $\Longleftrightarrow$ constraint                  |
| Bayesian     | Prior   $\Longleftrightarrow$ stochastic constraint prior |

--

<br><br>
<!--- The duality between stochastic constraint priors and their standard representation is analagous to the duality between penalty and constraint regularisation in optimisation.--->
There is a duality in the optimisation setting, *and for the priors in a Bayesian analysis*.

---

## General stochastic constraint framework

For continuous random variables:

$$p(\boldsymbol{\theta},\boldsymbol{\omega}) \propto f_{\tilde{\boldsymbol{\theta}}}(\boldsymbol{\theta})~f_{\tilde{\boldsymbol{\omega}}}(\boldsymbol{\omega})~1(\boldsymbol{r}(\boldsymbol{\theta}) \preceq \boldsymbol{\omega})$$

--

|||
|-------------------------|--------------|
| Base prior             | $\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}}, \quad \tilde{\boldsymbol{\theta}} \in \Theta \subseteq \mathbb{R}^{p}$ |
| Constraint variable      | $\tilde{\boldsymbol{\omega}} \sim f_{\tilde{\boldsymbol{\omega}}}, \quad \tilde{\boldsymbol{\omega}} \in \Omega \subseteq \mathbb{R}^{q}$ |
| Penalty function  | $\boldsymbol{r}(\boldsymbol{\theta}): \qquad \Theta \rightarrow \Omega$ |

--

**Key ideas**: 

- Truncation is applied *jointly* to base and constraint variables.

--

- Decomposition exposes marginal and multivariate components of the prior.

--

**Connections**: Scale mixture of normal/uniform, slice sampling, skew random variables, exponentially tilted random variables,...

<!---#### Base prior: 

$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}}, \quad \tilde{\boldsymbol{\theta}} \in \Theta \subseteq \mathbb{R}^{p}$.
    
#### Constraint variable: 

$\tilde{\boldsymbol{\omega}} \sim f_{\tilde{\boldsymbol{\omega}}}, \quad \tilde{\boldsymbol{\omega}} \in \Omega \subseteq \mathbb{R}^{q}$
    
#### Penalty function: 

$\boldsymbol{r}(\boldsymbol{\theta}): \Theta \rightarrow \Omega$ 


## Stochastic constraint framework

### Structure

$$(\boldsymbol{\theta}, \boldsymbol{\omega} \vert \boldsymbol{\lambda}) \overset{\text{d}}{=} (\tilde{\boldsymbol{\theta}}, \tilde{\boldsymbol{\omega}} \;\vert\;  \boldsymbol{r}(\tilde{\boldsymbol{\theta}}) \preceq \tilde{\boldsymbol{\omega}}, \boldsymbol{\lambda})$$

$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}}$$ 
$$(\tilde{\boldsymbol{\omega}} \vert \boldsymbol{\lambda}) \sim f_{\tilde{\boldsymbol{\omega}}  \vert \boldsymbol{\lambda}}$$

The joint support of the joint distribution $(\tilde{\boldsymbol{\theta}}, \tilde{\boldsymbol{\omega}})$ is constrained element-wise by the inequality $\boldsymbol{r}(\tilde{\boldsymbol{\theta}} ) \preceq \tilde{\boldsymbol{\omega}}$.-->


---

## Examples

```{r aqua-lasso-2, echo=FALSE, eval=TRUE,  out.width = 732, out.height = 488}
knitr::include_graphics("imgs/aquaman-lasso2.gif")
```

---

## 1D example

| Base prior | Penalty | Constraint |
|------------|---------|------------|
| $\text{N}(0,1)$ | $-$ | $-$ |

```{r sc-example-1, echo=FALSE, eval=TRUE, out.width = 600, out.height = 400, fig.align='center'}
print(plot_normal)
```

---

## 1D example

| Base prior | Penalty | Constraint |
|------------|---------|------------|
| $\text{N}(0,1)$ | $\vert\theta\vert$ | $\text{Exp}(1)$ |

```{r sc-example-2, echo=FALSE, eval=TRUE, out.width = 600, out.height = 400, fig.align='center'}
print(plot_normal_l1_exp)
```

---

## 1D example

| Base prior | Penalty | Constraint |
|------------|---------|------------|
| $\text{N}(0,1)$ | $\vert\theta\vert$ | $\text{Gamma}(0.5,1)$ |

```{r sc-example-3, echo=FALSE, eval=TRUE, out.width = 600, out.height = 400, fig.align='center'}
print(plot_normal_l1_gamma)
```

---

## Horseshoe prior

$$(\boldsymbol{\theta}, \boldsymbol{\omega} \;\vert\; \boldsymbol{\lambda}) \overset{\text{d}}{=} \left( \tilde{\boldsymbol{\theta}}, \tilde{\boldsymbol{\omega}} \;\vert\; \tilde{\theta}_{i}^{2} \leq \tilde{\omega}_{i} \;\forall i \in P, \boldsymbol{\lambda} \right)$$
$$\tilde{\boldsymbol{\theta}} \sim f_{\tilde{\boldsymbol{\theta}}} \propto 1$$
$$(\tilde{\omega}_{i}\vert\lambda_{i},\tau) \sim \text{Exp}(2^{-1} [\lambda_{i}\tau]^{-2})$$
$$\lambda_{i} \sim \text{Cauchy}_{+}(0,1)$$
$$\tau  \sim \text{Cauchy}_{+}(0,1)$$
$$\text{for } i \in P = \{1,2,3,\ldots,p\}$$
--

Similar representations exist for
- Scale-normal mixtures
- Regularised-horseshoe
- Dirichlet-Laplace
- R2-D2 prior

<!--- Add box around SC part to demo marginalisation?--->

---

## Stochastic nets

- A restricted family of stochastic constraint priors contain all(?) continuous shrinkage priors (horseshoe etc.) in the Bayesian literature.

- I refer to these as *stochastic nets*.

**... why?**

--

```{r aqua-flying-fish, echo=FALSE, eval=TRUE, fig.align='center'}
knitr::include_graphics("imgs/aquaman-flying-fish.gif")
```

---

## Visualisation of generative process

$$(x,y) \sim \text{N}(0,1)$$
$$\boldsymbol{\lambda} \sim \text{Dir}(1,1)$$

$$\tau \sim \text{Exp}(2)$$

$$\text{s.t. } \lambda_{1}\vert x \vert + \lambda_{2}\vert y \vert \leq \tau$$

```{r sc-reject-gif, echo=FALSE, eval=TRUE, fig.align='center'}
knitr::include_graphics("imgs/sc-rejection-viz1.gif")
```

---

## Visualisation of generative process

$$(x,y) \sim \text{N}(0,1)$$
$$\boldsymbol{\lambda} \sim \text{Dir}(1,1)$$

$$\tau \sim \text{Exp}(2)$$

$$\text{s.t. } \lambda_{1}\vert x \vert + \lambda_{2}\vert y \vert \leq \tau$$

```{r sc-reject-pause, echo=FALSE, eval=TRUE, fig.align='center'}
knitr::include_graphics("imgs/sc-rejection-viz1-pause.png")
```

---

## A stochastic net: constraining variables

```{r sc-reject-shape, echo=FALSE, eval=TRUE, fig.align='center', out.height=500, out.width=600}
knitr::include_graphics("imgs/sc-rej-shape-only.png")
```

---

## Gibbs samplers for continuous-shrinkage priors

**Normal model**: $(\boldsymbol{y} ~\vert~ \boldsymbol{X}, \boldsymbol{\beta},\sigma^2) \sim \text{N}(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2\boldsymbol{I})$.

--

### Ingredients of standard algorithm

--

(a) Sample $(\boldsymbol{\beta}~\vert~ \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{N}(\boldsymbol{\mu},\boldsymbol{V})$.
   - $\boldsymbol{\mu} = \boldsymbol{V}\boldsymbol{X}^{\top}\boldsymbol{y}$
   - $\boldsymbol{V} = (\boldsymbol{X}^{\top}\boldsymbol{X} + \boldsymbol{S}^{-1})^{-1}$
   - $\boldsymbol{S}$ = diagonal matrix from $\boldsymbol{\lambda}$ and $\tau$.

--

(b) Sample $(\sigma^2~\vert~\boldsymbol{\beta}, \boldsymbol{\lambda}, \tau) \sim \text{IG}(a_{1},a_{2})$

--

(c) Sample $(\boldsymbol{\lambda}~\vert~\boldsymbol{\beta}, \sigma^2, \tau)$

--

(d) Sample $(\tau~\vert~ \boldsymbol{\beta}, \boldsymbol{\lambda}, \sigma^2)$

--

**Q:** What is the computational bottleneck of this algorithm?

--

**A:** Inverting (or decomposing) $\boldsymbol{V}$ is $\mathcal{O}(p^3)$.

---

### Truncated-Gibbs sampler

**Aim**: Exploit the geometry by decomposing the prior to create a better Gibbs sampler.

--

(a1) Sample $(\boldsymbol{\omega}~\vert~\boldsymbol{\beta}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{sExp}(b_1)$

--

   - Use $\boldsymbol{\omega}$ (current magnitude) to choose appropriate sampling step, namely

--
   
   - Order $\boldsymbol{\omega}$ by size and split into two groups, $I$ and $J$. Where $\omega_{i} \leq \epsilon$ for $i \in I$.


--

(a2) Sample $(\beta_{i}~\vert~ \boldsymbol{\beta}_{(i)},\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{U}(-\omega_{i}^{1/\nu},\omega_{i}^{1/\nu})$ with MH correction for $i \in I$.

--

   - The dependence on $\boldsymbol{\beta}_{(i)}$ exists but is very small. You can argue that $(\beta_{i}~\vert~ \boldsymbol{\beta}_{(i)},\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau) \overset{d}{\approx} (\beta_{i}~\vert~\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau)$ if $\epsilon$ is small enough.

--

(a3) Sample $(\boldsymbol{\beta}_{J}~\vert~ \boldsymbol{\beta}_{I}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{N}(\boldsymbol{\mu}_{J|I},\boldsymbol{V}_{J|I})$.

--

   - Note: $\boldsymbol{\omega}$ marginalised out. Not necessary, but practical.


---

### Truncated-Gibbs sampler

**Aim**: Exploit the geometry by decomposing the prior to create a better Gibbs sampler.

(a1) Sample $(\boldsymbol{\omega}~\vert~\boldsymbol{\beta}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{sExp}(b_1)$

(a2) Sample $(\beta_{i}~\vert~ \boldsymbol{\beta}_{(i)},\boldsymbol{\omega}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{U}(-\omega_{i}^{1/\nu},\omega_{i}^{1/\nu})$ with MH correction for $i \in I$.

(a3) Sample $(\boldsymbol{\beta}_{J}~\vert~ \boldsymbol{\beta}_{I}, \boldsymbol{\lambda}, \sigma^2, \tau) \sim \text{N}(\boldsymbol{\mu}_{J|I},\boldsymbol{V}_{J|I})$.

--

**Complexity**: 

- $\mathcal{O}(\vert J \vert^{3} + p \vert I\vert)$ in each iteration.

--

- We can fix $\vert J \vert$ by selecting $\epsilon$ adaptively. Then the complexity is $\mathcal{O}(p^2)$.

---

- In high dimensions with shrinkage priors, the majority of coefficients are forced to be very close to zero.
 
--
 
- Understanding the structure and geometry of the problem is important!


--

Be like Aquaman!
<br><br>
```{r more-aqua, echo=FALSE, eval=TRUE, fig.align='center'}
knitr::include_graphics("imgs/aquaman-jetski.gif")
```

---

## Results on some toy problems

Model: Linear regression, iid Gaussian errors, using R2-D2 prior

Simulation settings:

$n = 100$

$p \in \{500,1000,2500,5000\}$

$\rho \in \{0.1,0.5\}$, controls correlation in $\boldsymbol{X}$

$\epsilon \in \{0.1,0.2\}$

$\boldsymbol{\beta} = [\boldsymbol{0}_{10}^{\top}~\boldsymbol{b}_{1}^{\top}~\boldsymbol{0}_{30}^{\top}~~\boldsymbol{b}_{2}^{\top}~\boldsymbol{0}_{p-50}^{\top}]^{\top}$

$\boldsymbol{b}_{1} = [2,~2,−5,−5,−5]$

$\boldsymbol{b}_{2} = [−2,−2,−2,~5,~5]$

$2000$ samples repeated independently 100 times.

---

### Mean (SD) computation time

```{r comp-viz, echo=FALSE, eval=TRUE, fig.align='center', out.height=500, out.width=500}
knitr::include_graphics("imgs/avg-computation-time.png")
```

---

### Posterior mean (SD) of coefficient SSE

```{r sse-table, echo=FALSE, eval=TRUE, fig.align='center', out.height=500}
knitr::include_graphics("imgs/table-sse.png")
```

---

### Grouping and uniform proposal

```{r accept-table, echo=FALSE, eval=TRUE, fig.align='center', out.height=500,out.width=550}
knitr::include_graphics("imgs/table-mh-accept.png")
```

---

<!---### Mean (SD) computation time

```{r time-table, echo=FALSE, eval=TRUE, fig.align='center', out.height=550,out.width=450}
knitr::include_graphics("imgs/table-time.png")
``` 
--->

## Conclusions

- One technique for scaling with dimension where sparsity is desired.

- Can be combined with other techniques

- Other areas under investigation with stochastic constraints:

   - Theoretical results
   
   - Discrete version

   - Prior construction
   
   - Variable selection from continuous-shrinkage prior
   
   - You tell me!
   
- Acknowledgements: 

   - Berwin Turlach, Kevin Murray, Chris Drovandi
   - Pawsey HPC

---

## (A random sample of) Key references

```{r bib-list, echo = FALSE, results='asis'}

s_cites <- sample(names(myBib), size = 6)

for(s in s_cites){
  
  p <- NoCite(bib = myBib,  s)
  
}

RefManageR::PrintBibliography(myBib)
```
